install.packages("devtools")
install.packages("keras")
install.packages("tidyverse")
library(keras)
?install_keras
install_keras(tensorflow="gpu")
mnist <- dataset_mnist()
library(keras)
library(tensorflow)
mnist <- dataset_mnist()
library(keras)
mnist <- dataset_mnist()
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
dim(x_train) <- c(nrow(x_train), 784)
dim(x_test) <- c(nrow(x_test), 784)
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
Sys.setenv(KERAS_BACKEND = "cntk")
library(keras)
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
dim(x_train) <- c(nrow(x_train), 784)
dim(x_test) <- c(nrow(x_test), 784)
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
plot(history)
loss_and_metrics <- model %>% evaluate(x_test, y_test)
classes <- model %>% predict_classes(x_test)
Sys.setenv(KERAS_BACKEND = "cntk")
library(keras)
test <- backend()
test <- backend()
Sys.setenv(KERAS_BACKEND = "tensorflow")
library(keras)
test <- backend()
mnist <- dataset_mnist()
library(keras)
mnist <- dataset_mnist()
test <- backend()
test
str(test)
Sys.setenv(KERAS_BACKEND = "cntk")
library(keras)
test <- backend()
test
Sys.setenv(KERAS_BACKEND = "cntk")
library(keras)
mnist <- dataset_mnist()
library(keras)
mnist <- dataset_mnist()
test <- backend()
test$backend()
Sys.setenv(KERAS_BACKEND = "cntk")
library(keras)
test <- backend()
test$backend()
?backend
Sys.getenv("LD_LIBRARY_PATH")
Sys.getenv("KERAS_BACKEND")
Sys.getenv("KERAS_PYTHON")
Sys.setenv(KERAS_PYTHON = "\\Users\\Claus\\Anaconda3")
Sys.getenv("KERAS_PYTHON")
Sys.getenv("KERAS_PYTHON")
Sys.setenv(KERAS_BACKEND = "cntk")
Sys.setenv(KERAS_PYTHON = "\\Users\\Claus\\Anaconda3")
Sys.getenv("KERAS_PYTHON")
library(keras)
test <- backend()
Sys.setenv(KERAS_PYTHON = "\\Users\\Claus\\Anaconda3\\")
library(keras)
test <- backend()
Sys.setenv(KERAS_PYTHON = "\\Users\\Claus\\Anaconda3\\envs\\cntk\\")
library(keras)
test <- backend()
Sys.setenv(KERAS_PYTHON = "\\Users\\Claus\\Anaconda3\\envs\\cntk\\")
library(keras)
test <- backend()
Sys.setenv(KERAS_BACKEND = "cntk")
library(keras)
Sys.setenv(KERAS_BACKEND = "cntk")
Sys.setenv(KERAS_PYTHON = "\\Users\\Claus\\Anaconda3\\envs\\cntk\\")
library(keras)
test <- backend()
test$backend()
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
dim(x_train) <- c(nrow(x_train), 784)
dim(x_test) <- c(nrow(x_test), 784)
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
plot(history)
loss_and_metrics <- model %>% evaluate(x_test, y_test)
classes <- model %>% predict_classes(x_test)
Sys.setenv(KERAS_BACKEND = "cntk")
Sys.setenv(KERAS_PYTHON = "\\Users\\Claus\\Anaconda3\\envs\\cntk\\")
library(keras)
model <- keras_model_sequential()
model %>%
layer_dense(units = 32, input_shape = c(784)) %>%
layer_activation('relu') %>%
layer_dense(units = 10) %>%
layer_activation('softmax')
summary(model)
model %>% compile(
optimizer = 'rmsprop',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
# add layers and compile the model
model %>%
layer_dense(units = 32, activation = 'relu', input_shape = c(100)) %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
compile(
optimizer = 'rmsprop',
loss = 'binary_crossentropy',
metrics = c('accuracy')
)
# Generate dummy data
data <- matrix(runif(1000*100), nrow = 1000, ncol = 100)
labels <- matrix(round(runif(1000, min = 0, max = 1)), nrow = 1000, ncol = 1)
# Train the model, iterating on the data in batches of 32 samples
model %>% fit(data, labels, epochs=10, batch_size=32)
model <- keras_model_sequential()
model %>%
layer_dense(units = 32, activation = 'relu', input_shape = c(100)) %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
compile(
optimizer = 'rmsprop',
loss = 'binary_crossentropy',
metrics = c('accuracy')
)
data <- matrix(runif(1000*100), nrow = 1000, ncol = 100)
labels <- matrix(round(runif(1000, min = 0, max = 1)), nrow = 1000, ncol = 1)
model %>% fit(data, labels, epochs=10, batch_size=32)
# create model
model <- keras_model_sequential()
# define and compile the model
model %>%
layer_dense(units = 32, activation = 'relu', input_shape = c(100)) %>%
layer_dense(units = 10, activation = 'softmax') %>%
compile(
optimizer = 'rmsprop',
loss = 'categorical_crossentropy',
metrics = c('accuracy')
)
# Generate dummy data
data <- matrix(runif(1000*100), nrow = 1000, ncol = 100)
labels <- matrix(round(runif(1000, min = 0, max = 9)), nrow = 1000, ncol = 1)
# Convert labels to categorical one-hot encoding
one_hot_labels <- to_categorical(labels, num_classes = 10)
# Train the model, iterating on the data in batches of 32 samples
model %>% fit(data, one_hot_labels, epochs=50, batch_size=64)
max_features = 20000
maxlen = 100
embedding_size = 128
kernel_size = 5
filters = 64
pool_size = 4
lstm_output_size = 70
batch_size = 30
epochs = 10
imdb <- dataset_imdb(num_words = max_features)
x_train <- imdb$train$x %>%
pad_sequences(maxlen = maxlen)
x_test <- imdb$test$x %>%
pad_sequences(maxlen = maxlen)
model <- keras_model_sequential()
model %>%
layer_embedding(max_features, embedding_size, input_length = maxlen) %>%
layer_dropout(0.25) %>%
layer_conv_1d(
filters,
kernel_size,
padding = "valid",
activation = "relu",
strides = 1
) %>%
layer_max_pooling_1d(pool_size) %>%
layer_lstm(lstm_output_size) %>%
layer_dense(1) %>%
layer_activation("sigmoid")
model %>% compile(
loss = "binary_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
model %>% fit(
x_train, imdb$train$y,
batch_size = batch_size,
epochs = epochs,
validation_data = list(x_test, imdb$test$y)
)
library(stringr)
library(purrr)
library(tokenizers)
install.packages("tokenizers")
library(tokenizers)
maxlen <- 40
path <- get_file(
'nietzsche.txt',
origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'
)
text <- read_lines(path) %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
library(readr)
text <- read_lines(path) %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
print(sprintf("corpus length: %d", length(text)))
chars <- text %>%
unique() %>%
sort()
print(sprintf("total chars: %d", length(chars)))
dataset <- map(
seq(1, length(text) - maxlen - 1, by = 3),
~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
)
dataset <- transpose(dataset)
X <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))
y <- array(0, dim = c(length(dataset$sentece), length(chars)))
for(i in 1:length(dataset$sentece)){
X[i,,] <- sapply(chars, function(x){
as.integer(x == dataset$sentece[[i]])
})
y[i,] <- as.integer(chars == dataset$next_char[[i]])
}
model <- keras_model_sequential()
model %>%
layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%
layer_dense(length(chars)) %>%
layer_activation("softmax")
optimizer <- optimizer_rmsprop(lr = 0.01)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer
)
sample_mod <- function(preds, temperature = 1){
preds <- log(preds)/temperature
exp_preds <- exp(preds)
preds <- exp_preds/sum(exp(preds))
rmultinom(1, 1, preds) %>%
as.integer() %>%
which.max()
}
for(iteration in 1:60){
cat(sprintf("iteration: %02d ---------------\n\n", iteration))
model %>% fit(
X, y,
batch_size = 128,
epochs = 1
)
for(diversity in c(0.2, 0.5, 1, 1.2)){
cat(sprintf("diversity: %f ---------------\n\n", diversity))
start_index <- sample(1:(length(text) - maxlen), size = 1)
sentence <- text[start_index:(start_index + maxlen - 1)]
generated <- ""
for(i in 1:400){
x <- sapply(chars, function(x){
as.integer(x == sentence)
})
dim(x) <- c(1, dim(x))
preds <- predict(model, x)
next_index <- sample_mod(preds, diversity)
next_char <- chars[next_index]
generated <- str_c(generated, next_char, collapse = "")
sentence <- c(sentence[-1], next_char)
}
cat(generated)
cat("\n\n")
}
}
for(iteration in 1:1){
cat(sprintf("iteration: %02d ---------------\n\n", iteration))
model %>% fit(
X, y,
batch_size = 128,
epochs = 20
)
for(diversity in c(0.2, 0.5, 1, 1.2)){
cat(sprintf("diversity: %f ---------------\n\n", diversity))
start_index <- sample(1:(length(text) - maxlen), size = 1)
sentence <- text[start_index:(start_index + maxlen - 1)]
generated <- ""
for(i in 1:400){
x <- sapply(chars, function(x){
as.integer(x == sentence)
})
dim(x) <- c(1, dim(x))
preds <- predict(model, x)
next_index <- sample_mod(preds, diversity)
next_char <- chars[next_index]
generated <- str_c(generated, next_char, collapse = "")
sentence <- c(sentence[-1], next_char)
}
cat(generated)
cat("\n\n")
}
}
setwd("~/github/website-hugo")
blogdown:::serve_site()
